---
title: Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private
  Stochastic Saddle-Point Problems extended abstract
section: Original Papers
abstract: We study the problem of differentially-private (DP) stochastic (convex-concave)
  saddle-points in the polyhedral setting. We propose $(\varepsilon, \delta)$-DP algorithms
  based on stochastic mirror descent that attain nearly dimension-independent convergence
  rates for the expected duality gap, a type of guarantee that was known before only
  for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives,
  our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$,
  where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional
  second-order-smoothness assumption, we improve the rate on the expected gap to $\sqrt{\log(d)/n}
  + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$. Under this additional assumption, we also
  show, by using bias-reduced gradient estimators, that the duality gap is bounded
  by $\log(d)/\sqrt{n} + \log(d)/[n\varepsilon]^{1/2}$ with constant success probability.
  This result provides evidence of the near-optimality of the approach. Finally, we
  show that combining our methods with acceleration techniques from online learning
  leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral
  setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth
  stochastic objectives, our algorithms attain an excess risk of $\sqrt{\log(d)/n}
  + \log(d)^{7/10}/[n\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness,
  we improve the rate to $\sqrt{\log(d)/n} + \log(d)/\sqrt{n\varepsilon}$. Instrumental
  to all of these results are various extensions of the classical Maurey Sparsification
  Lemma, which may be of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gonzalez24a
month: 0
tex_title: Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private
  Stochastic Saddle-Point Problems extended abstract
firstpage: 1982
lastpage: 1982
page: 1982-1982
order: 1982
cycles: false
bibtex_author: Gonzalez, Tomas and Guzman, Cristobal and Paquette, Courtney
author:
- given: Tomas
  family: Gonzalez
- given: Cristobal
  family: Guzman
- given: Courtney
  family: Paquette
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/gonzalez24a/gonzalez24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
