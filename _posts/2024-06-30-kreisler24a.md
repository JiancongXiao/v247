---
title: Accelerated Parameter-Free Stochastic Optimization
section: Original Papers
abstract: We propose a method that achieves near-optimal rates for \emph{smooth} stochastic
  convex optimization and requires essentially no prior knowledge of problem parameters.
  This improves on prior work which requires knowing at least the initial distance
  to optimality $d_0$.   Our method, \textsc{U-DoG}, combines \textsc{UniXGrad} (Kavis
  et al., 2019) and \textsc{DoG} (Ivgi et al., 2023) with novel iterate stabilization
  techniques. It requires only loose bounds on $d_0$ and the noise magnitude, provides
  high probability guarantees under sub-Gaussian noise, and is also near-optimal in
  the non-smooth case. Our experiments show consistent, strong performance on convex
  problems and mixed results on neural network training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kreisler24a
month: 0
tex_title: Accelerated Parameter-Free Stochastic Optimization
firstpage: 3257
lastpage: 3324
page: 3257-3324
order: 3257
cycles: false
bibtex_author: Kreisler, Itai and Ivgi, Maor and Hinder, Oliver and Carmon, Yair
author:
- given: Itai
  family: Kreisler
- given: Maor
  family: Ivgi
- given: Oliver
  family: Hinder
- given: Yair
  family: Carmon
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/kreisler24a/kreisler24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
