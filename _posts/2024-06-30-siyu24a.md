---
title: 'Training Dynamics of Multi-Head Softmax Attention for In-Context Learning:
  Emergence, Convergence, and Optimality (extended abstract)'
section: Original Papers
abstract: 'We study the dynamics of gradient flow for training a multi-head softmax
  attention model for in-context learning of multi-task linear regression.  We establish
  the global   convergence  of gradient flow under suitable choices of initialization.
  In addition,  we prove that  an interesting “task allocation" phenomenon emerges
  during the gradient flow dynamics, where each attention head focuses on solving
  a single task of the multi-task model. Specifically, we prove that the gradient
  flow dynamics can be split into three phases — a warm-up phase where the loss decreases
  rather slowly and the attention heads gradually build up their inclination towards
  individual tasks, an emergence phase where each head selects a single task and the
  loss rapidly decreases, and a convergence phase  where the attention parameters
  converge to a limit.  Furthermore, we prove the optimality of gradient flow in the
  sense that the limiting model learned by gradient flow is on par with the best possible
  multi-head softmax attention model up to a constant factor.  Our analysis also delineates
  a strict separation in terms of the prediction accuracy of ICL between single-head
  and multi-head attention models.  The key technique for our convergence  analysis
  is to map  the gradient flow dynamics in the parameter space to a set of ordinary
  differential equations in the spectral domain, where the relative magnitudes of
  the semi-singular values of the attention weights determines task allocation.   To
  our best knowledge, our work provides the first convergence result for the multi-head
  softmax attention model. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: siyu24a
month: 0
tex_title: 'Training Dynamics of Multi-Head Softmax Attention for In-Context Learning:
  Emergence, Convergence, and Optimality (extended abstract)'
firstpage: 4573
lastpage: 4573
page: 4573-4573
order: 4573
cycles: false
bibtex_author: Siyu, Chen and Heejune, Sheen and Tianhao, Wang and Zhuoran, Yang
author:
- given: Chen
  family: Siyu
- given: Sheen
  family: Heejune
- given: Wang
  family: Tianhao
- given: Yang
  family: Zhuoran
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/siyu24a/siyu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
