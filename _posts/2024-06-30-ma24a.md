---
title: Convergence of Gradient Descent with Small Initialization for Unregularized
  Matrix Completion
section: Original Papers
abstract: We study the problem of symmetric matrix completion, where the goal is to
  reconstruct a positive semidefinite matrix $X^\star \in \mathbb{R}^{d\times d}$
  of rank-$r$, parameterized by $UU^{\top}$, from only a subset of its observed entries.
  We show that the vanilla gradient descent (GD) with small initialization provably
  converges to the ground truth $X^\star$ without requiring any explicit regularization.
  This convergence result holds true even in the over-parameterized scenario, where
  the true rank $r$ is unknown and conservatively over-estimated by a search rank
  $r’\gg r$. The existing results for this problem either require explicit regularization,
  a sufficiently accurate initial point, or exact knowledge of the true rank $r$.  In
  the over-parameterized regime where $r’\geq r$, we show that, with $\widetilde\Omega(dr^9)$
  observations, GD with an initial point $\|U_0\| \leq O(\epsilon)$ converges near-linearly
  to an $\epsilon$-neighborhood of $X^\star$. Consequently, smaller initial points
  result in increasingly accurate solutions. Surprisingly, neither the convergence
  rate nor the final accuracy depends on the over-parameterized search rank $r’$,
  and they are only governed by the true rank $r$. In the exactly-parameterized regime
  where $r’=r$, we further enhance this result by proving that GD converges at a faster
  rate to achieve an arbitrarily small accuracy $\epsilon>0$, provided the initial
  point satisfies $\|U_0\| = O(1/d)$. At the crux of our method lies a novel weakly-coupled
  leave-one-out analysis, which allows us to establish the global convergence of GD,
  extending beyond what was previously possible using the classical leave-one-out
  analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma24a
month: 0
tex_title: Convergence of Gradient Descent with Small Initialization for Unregularized
  Matrix Completion
firstpage: 3683
lastpage: 3742
page: 3683-3742
order: 3683
cycles: false
bibtex_author: Ma, Jianhao and Fattahi, Salar
author:
- given: Jianhao
  family: Ma
- given: Salar
  family: Fattahi
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/ma24a/ma24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
