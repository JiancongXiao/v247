---
title: 'On the Growth of Mistakes in Differentially Private Online Learning: A Lower
  Bound Perspective'
section: Original Papers
abstract: " In this paper, we provide lower bounds for Differentially Private (DP)
  Online Learning algorithms. Our result shows that, for a broad class of $(\\epsilon,\\delta)$-DP
  online algorithms, for number of rounds $T$ such that $\\log T\\leq O\\left(1 /
  \\delta\\right)$, the expected number of mistakes incurred by the algorithm grows
  as \\(\\Omega\\left(\\log T\\right)\\). This matches the upper bound obtained by
  Golowich and Livni (2021) and is in contrast to non-private online learning where
  the number of mistakes is independent of \\(T\\).  To the best of our knowledge,
  our work is the first result towards settling lower bounds for DP–Online learning
  and partially addresses the open question in Sanyal and Ramponi (2022)."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dmitriev24a
month: 0
tex_title: 'On the Growth of Mistakes in Differentially Private Online Learning: A
  Lower Bound Perspective'
firstpage: 1379
lastpage: 1398
page: 1379-1398
order: 1379
cycles: false
bibtex_author: Dmitriev, Daniil and Szab{\'o}, Krist{\'o}f and Sanyal, Amartya
author:
- given: Daniil
  family: Dmitriev
- given: Kristóf
  family: Szabó
- given: Amartya
  family: Sanyal
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/dmitriev24a/dmitriev24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
