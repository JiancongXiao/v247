---
title: The Price of Adaptivity in Stochastic Convex Optimization
section: Original Papers
abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex
  optimization. Given a set of problem parameters we wish to adapt to, we define a
  “price of adaptivity” (PoA) that, roughly speaking, measures the multiplicative
  increase in suboptimality due to uncertainty in these parameters. When the initial
  distance to the optimum is unknown but a gradient norm bound is known, we show that
  the PoA is at least logarithmic for expected suboptimality, and double-logarithmic
  for median suboptimality. When there is uncertainty in both distance and gradient
  norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower
  bounds nearly match existing upper bounds, and establish that there is no parameter-free
  lunch.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: carmon24a
month: 0
tex_title: The Price of Adaptivity in Stochastic Convex Optimization
firstpage: 772
lastpage: 774
page: 772-774
order: 772
cycles: false
bibtex_author: Carmon, Yair and Hinder, Oliver
author:
- given: Yair
  family: Carmon
- given: Oliver
  family: Hinder
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/carmon24a/carmon24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
