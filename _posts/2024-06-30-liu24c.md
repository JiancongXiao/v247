---
title: 'The Predicted-Updates Dynamic Model: Offline, Incremental, and Decremental
  to Fully Dynamic Transformations'
section: Original Papers
abstract: The main bottleneck in designing efficient dynamic algorithms is the unknown
  nature of the update sequence.  In particular, there are problems where the separation
  in runtime between the best offline or partially dynamic solutions and the best
  fully dynamic solutions is polynomial, sometimes even exponential. In this paper,
  we formulate the \emph{predicted-updates dynamic model}, one of the first \emph{beyond-worst-case}
  models for dynamic algorithms, which generalizes a large set of well-studied dynamic
  models including the offline dynamic, incremental, and decremental models to the
  fully dynamic setting when given predictions about the update times of the elements.
  Our paper models real world settings, in which we often have access to side information
  that allows us to make coarse predictions about future updates.  We formulate a
  framework that bridges the gap between fully and offline/partially dynamic, leading
  to greatly improved runtime bounds over the state-of-the-art dynamic algorithms
  for a variety of important problems such as triconnectivity, planar digraph all
  pairs shortest paths, \(k\)-edge connectivity, and others, for prediction error
  of reasonable magnitude. Our simple framework avoids heavy machinery, potentially
  leading to a new set of dynamic algorithms that are implementable in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24c
month: 0
tex_title: 'The Predicted-Updates Dynamic Model: Offline, Incremental, and Decremental
  to Fully Dynamic Transformations'
firstpage: 3582
lastpage: 3641
page: 3582-3641
order: 3582
cycles: false
bibtex_author: Liu, Quanquan C. and Srinivas, Vaidehi
author:
- given: Quanquan C.
  family: Liu
- given: Vaidehi
  family: Srinivas
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/liu24c/liu24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
