---
title: Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning
  with Few Actions
section: Original Papers
abstract: One of the most natural approaches to reinforcement learning (RL) with function
  approximation is value iteration, which inductively generates approximations to
  the optimal value function by solving a sequence of regression problems. To ensure
  the success of value iteration, it is typically assumed that Bellman completeness
  holds, which ensures that these regression problems are well- specified. We study
  the problem of learning an optimal policy under Bellman completeness in the online
  model of RL with linear function approximation. In the linear setting, while statistically
  efficient algorithms are known under Bellman completeness (e.g., (Jiang et al.,
  2017; Zanette et al., 2020a)), these algorithms all rely on the principle of global
  optimism which requires solving a nonconvex optimization problem. In particular,
  it has remained open as to whether computationally efficient algorithms exist. In
  this paper we give the first polynomial-time algorithm for RL under linear Bellman
  completeness when the number of actions is any constant.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: golowich24a
month: 0
tex_title: Linear Bellman Completeness Suffices for Efficient Online Reinforcement
  Learning with Few Actions
firstpage: 1939
lastpage: 1981
page: 1939-1981
order: 1939
cycles: false
bibtex_author: Golowich, Noah and Moitra, Ankur
author:
- given: Noah
  family: Golowich
- given: Ankur
  family: Moitra
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/golowich24a/golowich24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
