---
title: A faster and simpler algorithm for learning shallow networks
section: Original Papers
abstract: We revisit the well-studied problem of learning a linear combination of
  $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional
  Gaussian measure. Chen et al. recently gave the first algorithm for this problem
  to run in $\mathrm{poly}(d,1/\epsilon)$ time when $k = O(1)$, where $\epsilon$ is
  the target error. More precisely, their algorithm runs in time $(d/\epsilon)^{\mathrm{quasipoly}(k)}$
  and learns over multiple stages. Here we show that a much simpler one-stage version
  of their algorithm suffices, and moreover its runtime is only $(d k/\epsilon)^{O(k^2)}$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24b
month: 0
tex_title: A faster and simpler algorithm for learning shallow networks
firstpage: 981
lastpage: 994
page: 981-994
order: 981
cycles: false
bibtex_author: Chen, Sitan and Narayanan, Shyam
author:
- given: Sitan
  family: Chen
- given: Shyam
  family: Narayanan
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/chen24b/chen24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
