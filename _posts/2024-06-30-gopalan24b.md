---
title: Omnipredictors for regression and the approximate rank of convex functions
section: Original Papers
abstract: 'Consider the supervised learning setting where the goal is to learn to
  predict labels $\mathbf y$ given points $\mathbf x$ from a distribution. An \textit{omnipredictor}
  for a class $\mathcal L$ of loss functions and a class $\mathcal C$ of hypotheses
  is a predictor whose predictions incur less expected loss than the best hypothesis
  in $\mathcal C$ for every loss in $\mathcal L$. Since the work of Gopalan et al.
  (2021) that introduced the notion, there has been a large body of work in the setting
  of binary labels where $\mathbf y \in \{0, 1\}$, but much less is known about the
  regression setting where $\mathbf y \in [0,1]$ can be continuous. The naive generalization
  of the previous approaches to regression is to predict the probability distribution
  of $y$, discretized to $\varepsilon$-width intervals. The running time would be
  exponential in the size of the output of the omnipredictor, which is $1/\varepsilon$.
  Our main conceptual contribution is the notion of \textit{sufficient statistics}
  for loss minimization over a family of loss functions: these are a set of statistics
  about a distribution such that knowing them allows one to take actions that minimize
  the expected loss for any loss in the family. The notion of sufficient statistics
  relates directly to the approximate rank of the family of loss functions. Thus,
  improved bounds on the latter yield improved runtimes for learning omnipredictors.
  Our key technical contribution is a bound of $O(1/\varepsilon^{2/3})$ on the $\epsilon$-approximate
  rank of convex, Lipschitz functions on the interval $[0,1]$, which we show is tight
  up to a factor of $\mathrm{polylog} (1/\epsilon)$.  This yields improved runtimes
  for learning omnipredictors for the class of all convex, Lipschitz loss functions
  under weak learnability assumptions about the class $\mathcal C$. We also give efficient
  omnipredictors when the loss families have low-degree polynomial approximations,
  or arise from generalized linear models (GLMs). This translation from sufficient
  statistics to faster omnipredictors is made possible by lifting the technique of
  loss outcome indistinguishability introduced by Gopalan et al. (2023a) for Boolean
  labels to the regression setting.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gopalan24b
month: 0
tex_title: Omnipredictors for regression and the approximate rank of convex functions
firstpage: 2027
lastpage: 2070
page: 2027-2070
order: 2027
cycles: false
bibtex_author: Gopalan, Parikshit and Okoroafor, Princewill and Raghavendra, Prasad
  and Sherry, Abhishek and Singhal, Mihir
author:
- given: Parikshit
  family: Gopalan
- given: Princewill
  family: Okoroafor
- given: Prasad
  family: Raghavendra
- given: Abhishek
  family: Sherry
- given: Mihir
  family: Singhal
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/gopalan24b/gopalan24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
