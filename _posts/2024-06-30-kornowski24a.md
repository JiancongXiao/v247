---
title: 'Open Problem: Anytime Convergence Rate of Gradient Descent'
section: Open Problems
abstract: 'Recent results show that vanilla gradient descent can be accelerated for
  smooth convex objectives, merely by changing the stepsize sequence. We show that
  this can lead to surprisingly large errors indefinitely, and therefore ask: Is there
  any stepsize schedule for gradient descent that accelerates the classic $\mathcal{O}(1/T)$
  convergence rate, at \emph{any} stopping time $T$?'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kornowski24a
month: 0
tex_title: 'Open Problem: Anytime Convergence Rate of Gradient Descent'
firstpage: 5335
lastpage: 5339
page: 5335-5339
order: 5335
cycles: false
bibtex_author: Kornowski, Guy and Shamir, Ohad
author:
- given: Guy
  family: Kornowski
- given: Ohad
  family: Shamir
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/kornowski24a/kornowski24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
