---
title: Pruning is Optimal for Learning Sparse Features in High-Dimensions
section: Original Papers
abstract: While it is commonly observed in practice that pruning networks to a certain
  level of sparsity can improve the quality of the features, a theoretical explanation
  of this phenomenon remains elusive.  In this work, we investigate this by demonstrating
  that a broad class of statistical models can be optimally learned using pruned neural
  networks trained with gradient descent, in high-dimensions.  We consider learning
  both single-index and multi-index models of the form $y = \sigma^*(\boldsymbol{V}^{\top}
  \boldsymbol{x}) + \epsilon$,  where $\sigma^*$  is a degree-$p$ polynomial,  and
  $\boldsymbol{V} \in \mathbbm{R}^{d \times r}$ with $r \ll d$, is the matrix containing
  relevant model directions. We assume that $\boldsymbol{V}$ satisfies a certain $\ell_q$-sparsity
  condition for matrices and show that pruning neural networks proportional to the
  sparsity level of $\boldsymbol{V}$ improves their sample complexity compared to
  unpruned networks.  Furthermore, we establish  Correlational Statistical Query (CSQ)
  lower bounds in this setting, which take the sparsity level of $\boldsymbol{V}$
  into account. We show that if the sparsity level of $\boldsymbol{V}$ exceeds a certain
  threshold, training pruned networks with a gradient descent algorithm achieves the
  sample complexity suggested by the CSQ lower bound.  In the same scenario, however,  our
  results imply that basis-independent methods such as models trained via standard
  gradient descent initialized with rotationally invariant random weights can provably
  achieve only suboptimal sample complexity.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vural24a
month: 0
tex_title: Pruning is Optimal for Learning Sparse Features in High-Dimensions
firstpage: 4787
lastpage: 4861
page: 4787-4861
order: 4787
cycles: false
bibtex_author: Vural, Nuri Mert and Erdogdu, Murat A
author:
- given: Nuri Mert
  family: Vural
- given: Murat A
  family: Erdogdu
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/vural24a/vural24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
