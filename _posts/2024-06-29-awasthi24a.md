---
title: Learning Neural Networks with Sparse Activations
section: Original Papers
abstract: A core component present in many successful neural network architectures,
  is an MLP block of two fully connected layers with a non-linear activation in between.
  An intriguing phenomenon observed empirically, including in transformer architectures,
  is that, after training, the activations in the hidden layer of this MLP block tend
  to be extremely sparse on any given input. Unlike traditional forms of sparsity,
  where there are neurons/weights which can be deleted from the network, this form
  of {\em dynamic} activation sparsity appears to be harder to exploit to get more
  efficient networks.  Motivated by this we initiate a formal study of PAC learnability
  of MLP layers that exhibit activation sparsity. We present a variety of results
  showing that such classes of functions do lead to provable computational and statistical
  advantages over their non-sparse counterparts. Our hope is that a better theoretical
  understanding of {\em sparsely activated} networks would lead to methods that can
  exploit activation sparsity in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: awasthi24a
month: 0
tex_title: Learning Neural Networks with Sparse Activations
firstpage: 406
lastpage: 425
page: 406-425
order: 406
cycles: false
bibtex_author: Awasthi, Pranjal and Dikkala, Nishanth and Kamath, Pritish and Meka,
  Raghu
author:
- given: Pranjal
  family: Awasthi
- given: Nishanth
  family: Dikkala
- given: Pritish
  family: Kamath
- given: Raghu
  family: Meka
date: 2024-06-29
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 29
pdf: https://proceedings.mlr.press/v247/awasthi24a/awasthi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
