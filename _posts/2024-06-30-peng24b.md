---
title: The sample complexity of multi-distribution learning
section: Original Papers
abstract: " Multi-distribution learning generalizes the classic PAC learning to handle
  data coming from multiple distributions. Given a set of $k$ data distributions and
  a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes
  the maximum population loss over $k$ distributions, up to $\\epsilon$ additive error.
  In this paper, we settle the sample complexity of multi-distribution learning by
  giving an algorithm of sample complexity $\\widetilde{O}((d+k)\\epsilon^{-2}) \\cdot
  (k/\\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor
  and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peng24b
month: 0
tex_title: The sample complexity of multi-distribution learning
firstpage: 4185
lastpage: 4204
page: 4185-4204
order: 4185
cycles: false
bibtex_author: Peng, Binghui
author:
- given: Binghui
  family: Peng
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/peng24b/peng24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
