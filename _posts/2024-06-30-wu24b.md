---
title: 'Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the
  Loss Improves Optimization Efficiency'
section: Original Papers
abstract: We consider \emph{gradient descent} (GD) with a constant stepsize applied
  to logistic regression with linearly separable data, where the constant stepsize
  $\eta$ is so large that the loss initially oscillates. We show that GD exits this
  initial oscillatory phase rapidly â€” in $O(\eta)$ steps, and subsequently achieves
  an $\tilde{O}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results
  imply that, given a budget of $T$ steps, GD can achieve an \emph{accelerated} loss
  of $\tilde{O}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any
  use of momentum or variable stepsize schedulers. Our proof technique is versatile
  and also handles general classification loss functions (where exponential tails
  are needed for the $\tilde{O}(1/T^2)$ acceleration), nonlinear predictors in the
  \emph{neural tangent kernel} regime, and online \emph{stochastic gradient descent}
  (SGD) with a large stepsize, under suitable separability conditions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu24b
month: 0
tex_title: 'Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of
  the Loss Improves Optimization Efficiency'
firstpage: 5019
lastpage: 5073
page: 5019-5073
order: 5019
cycles: false
bibtex_author: Wu, Jingfeng and Bartlett, Peter L. and Telgarsky, Matus and Yu, Bin
author:
- given: Jingfeng
  family: Wu
- given: Peter L.
  family: Bartlett
- given: Matus
  family: Telgarsky
- given: Bin
  family: Yu
date: 2024-06-30
address:
container-title: Proceedings of Thirty Seventh Conference on Learning Theory
volume: '247'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 30
pdf: https://proceedings.mlr.press/v247/wu24b/wu24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
